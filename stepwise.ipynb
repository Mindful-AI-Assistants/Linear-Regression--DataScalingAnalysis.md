{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 # Stepwise Logistic Regression in Python\
\
# Import necessary libraries\
import pandas as pd\
import numpy as np\
import statsmodels.api as sm\
from sklearn.model_selection import train_test_split\
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\
import matplotlib.pyplot as plt\
\
# Load the dataset\
# Replace 'binary.csv' with your dataset filename\
data = pd.read_csv('binary.csv')\
\
# Define the target variable and features\
target = 'admit'  # Replace with your target column name\
features = data.columns.drop(target)\
\
X = data[features]\
y = data[target]\
\
# Split the dataset into training and testing sets\
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\
\
# Define the stepwise selection function\
def stepwise_selection(X, y, initial_list=[], threshold_in=0.01, threshold_out=0.05, verbose=True):\
    """\
    Perform a forward-backward feature selection based on p-value from statsmodels.api.OLS\
    Arguments:\
        X - pandas.DataFrame with candidate features\
        y - list-like with the target\
        initial_list - list of features to start with (column names of X)\
        threshold_in - include a feature if its p-value < threshold_in\
        threshold_out - exclude a feature if its p-value > threshold_out\
        verbose - whether to print the sequence of inclusions and exclusions\
    Returns:\
        list of selected features\
    """\
    included = list(initial_list)\
    while True:\
        changed = False\
        # forward step\
        excluded = list(set(X.columns) - set(included))\
        new_pval = pd.Series(index=excluded, dtype=float)\
        for new_column in excluded:\
            model = sm.Logit(y, sm.add_constant(pd.DataFrame(X[included + [new_column]]))).fit(disp=0)\
            new_pval[new_column] = model.pvalues[new_column]\
        if not new_pval.empty:\
            best_pval = new_pval.min()\
            if best_pval < threshold_in:\
                best_feature = new_pval.idxmin()\
                included.append(best_feature)\
                changed = True\
                if verbose:\
                    print(f'Add  \{best_feature:30\} with p-value \{best_pval:.6\}')\
\
        # backward step\
        model = sm.Logit(y, sm.add_constant(pd.DataFrame(X[included]))).fit(disp=0)\
        # use all coefs except intercept\
        pvalues = model.pvalues.iloc[1:]\
        worst_pval = pvalues.max()  # null if pvalues is empty\
        if worst_pval > threshold_out:\
            worst_feature = pvalues.idxmax()\
            included.remove(worst_feature)\
            changed = True\
            if verbose:\
                print(f'Drop \{worst_feature:30\} with p-value \{worst_pval:.6\}')\
        if not changed:\
            break\
    return included\
\
# Apply the stepwise selection\
selected_features = stepwise_selection(X_train, y_train)\
print("\\nSelected variables by Stepwise method:")\
print(selected_features)\
\
# Fit the final model with selected features\
X_train_selected = X_train[selected_features]\
X_test_selected = X_test[selected_features]\
model_final = sm.Logit(y_train, sm.add_constant(X_train_selected)).fit()\
\
# Display the summary of the final model\
print("\\nFinal Model Summary:")\
print(model_final.summary())\
\
# Display the final model equation\
print("\\nFinal Model Equation:")\
params = model_final.params\
equation = f"logit(P) = \{params[0]:.4f\} "\
for i in range(1, len(params)):\
    equation += f"+ (\{params[i]:.4f\} * \{params.index[i]\}) "\
print(equation)\
\
# Evaluate the model\
# Predict probabilities on the test set\
y_pred_prob = model_final.predict(sm.add_constant(X_test_selected))\
y_pred_class = (y_pred_prob > 0.5).astype(int)\
\
# Confusion Matrix\
conf_matrix = confusion_matrix(y_test, y_pred_class)\
print("\\nConfusion Matrix:")\
print(conf_matrix)\
\
# Classification Report\
print("\\nClassification Report:")\
print(classification_report(y_test, y_pred_class))\
\
# ROC Curve and AUC\
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\
roc_auc = auc(fpr, tpr)\
\
# Plot the ROC Curve\
plt.figure()\
plt.plot(fpr, tpr, color='cyan', label=f'ROC Curve (AUC = \{roc_auc:.2f\})')\
plt.plot([0, 1], [0, 1], color='red', linestyle='--')\
plt.xlabel('False Positive Rate')\
plt.ylabel('True Positive Rate')\
plt.title('ROC Curve')\
plt.legend(loc='lower right')\
plt.style.use('dark_background')  # Dark mode theme\
plt.show()}